{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import string\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "import yaml\n",
    "import ast\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, BaggingClassifier, ExtraTreesClassifier, \n",
    "                              HistGradientBoostingClassifier, StackingClassifier, VotingClassifier)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import (balanced_accuracy_score, f1_score,\n",
    "                             accuracy_score, confusion_matrix)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from src.constants import *\n",
    "from src.utils import load_yaml\n",
    "os.chdir('f:\\\\iNeuron\\\\Projects\\\\Scania_Truck_Failures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\iNeuron\\\\Projects\\\\Scania_Truck_Failures'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The student (Raj) scored 90.2 in Science\n"
     ]
    }
   ],
   "source": [
    "class greeting:\n",
    "    def __init__(self,\n",
    "                 name: str,\n",
    "                 marks: float):\n",
    "        self.name = name\n",
    "        self.marks = marks\n",
    "    def greeting_function(self, *args):\n",
    "        if args:\n",
    "            self.subject = args[0]\n",
    "        else:\n",
    "            self.subject = \"Maths\"\n",
    "        print (f\"The student ({self.name}) scored {self.marks} in {self.subject}\")\n",
    "\n",
    "obj = greeting(name='Raj', marks=90.2)\n",
    "obj.greeting_function('Science')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking train_data, test_data values and sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa_000</th>\n",
       "      <th>ab_000</th>\n",
       "      <th>ac_000</th>\n",
       "      <th>ad_000</th>\n",
       "      <th>ae_000</th>\n",
       "      <th>af_000</th>\n",
       "      <th>ag_000</th>\n",
       "      <th>ag_001</th>\n",
       "      <th>ag_002</th>\n",
       "      <th>ag_003</th>\n",
       "      <th>...</th>\n",
       "      <th>ee_002</th>\n",
       "      <th>ee_003</th>\n",
       "      <th>ee_004</th>\n",
       "      <th>ee_005</th>\n",
       "      <th>ee_006</th>\n",
       "      <th>ee_007</th>\n",
       "      <th>ee_008</th>\n",
       "      <th>ee_009</th>\n",
       "      <th>ef_000</th>\n",
       "      <th>eg_000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76698.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.130706e+09</td>\n",
       "      <td>280.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1240520.0</td>\n",
       "      <td>493384.0</td>\n",
       "      <td>721044.0</td>\n",
       "      <td>469792.0</td>\n",
       "      <td>339156.0</td>\n",
       "      <td>157956.0</td>\n",
       "      <td>73224.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33058.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>421400.0</td>\n",
       "      <td>178064.0</td>\n",
       "      <td>293306.0</td>\n",
       "      <td>245416.0</td>\n",
       "      <td>133654.0</td>\n",
       "      <td>81140.0</td>\n",
       "      <td>97576.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41040.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.280000e+02</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>277378.0</td>\n",
       "      <td>159812.0</td>\n",
       "      <td>423992.0</td>\n",
       "      <td>409564.0</td>\n",
       "      <td>320746.0</td>\n",
       "      <td>158022.0</td>\n",
       "      <td>95128.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.000000e+01</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>...</td>\n",
       "      <td>240.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60874.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.368000e+03</td>\n",
       "      <td>458.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>622012.0</td>\n",
       "      <td>229790.0</td>\n",
       "      <td>405298.0</td>\n",
       "      <td>347188.0</td>\n",
       "      <td>286954.0</td>\n",
       "      <td>311560.0</td>\n",
       "      <td>433954.0</td>\n",
       "      <td>1218.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 171 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa_000  ab_000        ac_000  ad_000  ae_000  af_000  ag_000  ag_001  \\\n",
       "0  76698.0     NaN  2.130706e+09   280.0     0.0     0.0     0.0     0.0   \n",
       "1  33058.0     NaN  0.000000e+00     NaN     0.0     0.0     0.0     0.0   \n",
       "2  41040.0     NaN  2.280000e+02   100.0     0.0     0.0     0.0     0.0   \n",
       "3     12.0     0.0  7.000000e+01    66.0     0.0    10.0     0.0     0.0   \n",
       "4  60874.0     NaN  1.368000e+03   458.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   ag_002  ag_003  ...     ee_002    ee_003    ee_004    ee_005    ee_006  \\\n",
       "0     0.0     0.0  ...  1240520.0  493384.0  721044.0  469792.0  339156.0   \n",
       "1     0.0     0.0  ...   421400.0  178064.0  293306.0  245416.0  133654.0   \n",
       "2     0.0     0.0  ...   277378.0  159812.0  423992.0  409564.0  320746.0   \n",
       "3     0.0   318.0  ...      240.0      46.0      58.0      44.0      10.0   \n",
       "4     0.0     0.0  ...   622012.0  229790.0  405298.0  347188.0  286954.0   \n",
       "\n",
       "     ee_007    ee_008  ee_009  ef_000  eg_000  \n",
       "0  157956.0   73224.0     0.0     0.0     0.0  \n",
       "1   81140.0   97576.0  1500.0     0.0     0.0  \n",
       "2  158022.0   95128.0   514.0     0.0     0.0  \n",
       "3       0.0       0.0     0.0     4.0    32.0  \n",
       "4  311560.0  433954.0  1218.0     0.0     0.0  \n",
       "\n",
       "[5 rows x 171 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"F:\\iNeuron\\Projects\\scania_failures_2\\\\artifacts\\data\\processed\\stage_1_processing\\preprocessed_train_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7500, 170) (2500, 170) (7500,) (2500,)\n",
      "class\n",
      "0    7351\n",
      "1     149\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "0    2454\n",
      "1      46\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x = df.iloc[:10000,:].drop(columns='class')\n",
    "y = df.iloc[:10000,:]['class']\n",
    "# skf = StratifiedKFold()\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y, test_size = 0.25, random_state=8)\n",
    "print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Params.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-14 14:46:20,392: INFO: utils: params.yaml yaml_file is loaded]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConfigBox({'optuna': {'Logistic_Regression': {'penalty': \"trial.suggest_categorical('penalty', ['l2', None])\"}, 'SGD_Classifier': {'loss': \"trial.suggest_categorical('loss', ['squared_epsilon_insensitive', 'epsilon_insensitive', 'huber', 'squared_error', 'perceptron', 'squared_hinge', 'hinge', 'log_loss', 'modified_huber'])\"}, 'Random Forest': {'n_estimators': \"trial.suggest_int('n_estimators', 100, 1000)\", 'criterion': \"trial.suggest_categorical('criterion', ['log_loss', 'entropy', 'gini'])\", 'max_features': \"trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\", 'class_weight': \"trial.suggest_categorical('class_weight', ['balanced', 'balanced_subsample'])\"}, 'Ada_Boost': {'n_estimators': \"trial.suggest_int('n_estimators', 100, 1000)\", 'algorithm': \"trial.suggest_categorical('algorithm', ['SAMME', 'SAMME.R'])\"}, 'Grad_Boost': {'loss': \"trial.suggest_categorical('loss', ['log_loss', 'exponential'])\", 'n_estimators': \"trial.suggest_int('n_estimators', 100, 1000)\", 'criterion': \"trial.suggest_categorical('criterion', ['friedman_mse', 'squared_error'])\", 'max_features': \"trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\"}, 'Bagging_Classifier': {'n_estimators': \"trial.suggest_int('n_estimators', 100, 1000)\"}, 'ExtraTreesClassifier': {'n_estimators': \"trial.suggest_int('n_estimators', 100, 1000)\", 'criterion': \"trial.suggest_categorical('criterion', ['log_loss', 'entropy', 'gini'])\", 'max_features': \"trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\", 'class_weight': \"trial.suggest_categorical('class_weight', ['balanced', 'balanced_subsample'])\"}, 'Hist_Grad_Boost_Classifier': {'max_iter': \"trial.suggest_int('max_iter', 100, 800)\"}, 'Decision_Tree_Classifier': {'criterion': \"trial.suggest_categorical('criterion', ['log_loss', 'entropy', 'gini'])\", 'splitter': \"trial.suggest_categorical('splitter', ['best', 'random'])\", 'max_features': \"trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\"}, 'XGB_Classifier': {'n_estimators': \"trial.suggest_int('n_estimators', 100, 1000)\", 'learning_rate': \"trial.suggest_float('learning_rate', .00001, 8.0)\", 'booster': \"trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart'])\", 'tree_method': \"trial.suggest_categorical('tree_method', ['exact', 'approx', 'hist'])\"}, 'KNN_Classifier': {'n_neighbors': \"trial.suggest_int('n_neighbors', 3, 11, step=2)\", 'weights': \"trial.suggest_categorical('weights', ['uniform', 'distance'])\", 'algorithm': \"trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute'])\"}, 'MLP_Classifier': {'hidden_layer_sizes': \"trial.suggest_categorical('hidden_layer_sizes', [(500,), (500, 300, 200, 150,), (700, 500, 300, 100, ), (1500, 800, 400, 200, )])\", 'activation': \"trial.suggest_categorical('activation', ['identity', 'logistic', 'tanh' , 'relu'])\", 'learning_rate': \"trial.suggest_categorical('learning_rate', ['constant', 'invscaling', 'adaptive'])\", 'max_iter': \"trial.suggest_int('max_iter', 100, 800)\"}, 'Stacked_Classifier': {'stack_method': \"trial.suggest_categorical('stack_method', ['auto', 'predict'])\", 'passthrough': \"trial.suggest_categorical('passthrough', [True, False])\"}}, 'hyperopt': {'Logistic_Regression': {'penalty': \"hp.choice('penalty', ['l2', None])\"}, 'SGD_Classifier': {'loss': \"hp.choice('loss', ['squared_epsilon_insensitive', 'epsilon_insensitive', 'huber', 'squared_error', 'perceptron', 'squared_hinge', 'hinge', 'log_loss', 'modified_huber'])\"}, 'Random Forest': {'n_estimators': \"scope.int(hp.quniform('n_estimators', 100, 1000, 1))\", 'criterion': \"hp.choice('criterion', ['log_loss', 'entropy', 'gini'])\", 'max_features': \"hp.choice('max_features', ['sqrt', 'log2', None])\", 'class_weight': \"hp.choice('class_weight', ['balanced', 'balanced_subsample'])\"}, 'Ada_Boost': {'n_estimators': \"scope.int(hp.quniform('n_estimators', 100, 1000, 1))\", 'algorithm': \"hp.choice('algorithm', ['SAMME', 'SAMME.R'])\"}, 'Grad_Boost': {'loss': \"hp.choice('loss', ['log_loss', 'exponential'])\", 'n_estimators': \"scope.int(hp.quniform('n_estimators', 100, 1000, 1))\", 'criterion': \"hp.choice('criterion', ['friedman_mse', 'squared_error'])\", 'max_features': \"hp.choice('max_features', ['sqrt', 'log2', None])\"}, 'Bagging_Classifier': {'n_estimators': \"scope.int(hp.quniform('n_estimators', 100, 1000, 1))\"}, 'ExtraTreesClassifier': {'n_estimators': \"scope.int(hp.quniform('n_estimators', 100, 1000, 1))\", 'criterion': \"hp.choice('criterion', ['log_loss', 'entropy', 'gini'])\", 'max_features': \"hp.choice('max_features', ['sqrt', 'log2', None])\", 'class_weight': \"hp.choice('class_weight', ['balanced', 'balanced_subsample'])\"}, 'Hist_Grad_Boost_Classifier': {'max_iter': \"scope.int(hp.quniform('max_iter', 100, 800, 1))\"}, 'Decision_Tree_Classifier': {'criterion': \"hp.choice('criterion', ['log_loss', 'entropy', 'gini'])\", 'splitter': \"hp.choice('splitter', ['best', 'random'])\", 'max_features': \"hp.choice('max_features', ['sqrt', 'log2', None])\"}, 'XGB_Classifier': {'n_estimators': \"scope.int(hp.quniform('n_estimators', 100, 1000, 1))\", 'learning_rate': \"hp.uniform('learning_rate', .00001, 8.0)\", 'booster': \"hp.choice('booster', ['gbtree', 'gblinear', 'dart'])\", 'tree_method': \"hp.choice('tree_method', ['exact', 'approx', 'hist'])\"}, 'KNN_Classifier': {'n_neighbors': \"scope.int(hp.quniform('n_neighbors', 3, 11, 2))\", 'weights': \"hp.choice('weights', ['uniform', 'distance'])\", 'algorithm': \"hp.choice('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute'])\"}, 'MLP_Classifier': {'hidden_layer_sizes': \"hp.choice('hidden_layer_sizes', [(500,), (500, 300, 200, 150,), (700, 500, 300, 100, ), (1500, 800, 400, 200, )])\", 'activation': \"hp.choice('activation', ['identity', 'logistic', 'tanh' , 'relu'])\", 'learning_rate': \"hp.choice('learning_rate', ['constant', 'invscaling', 'adaptive'])\", 'max_iter': \"scope.int(hp.quniform('max_iter', 100, 800, 1))\"}, 'Stacked_Classifier': {'stack_method': \"hp.choice('stack_method', ['auto', 'predict'])\", 'passthrough': \"hp.choice('passthrough', [True, False])\"}}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_config = load_yaml(PARAMS_PATH)\n",
    "params_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "BoxKeyError",
     "evalue": "\"'ConfigBox' object has no attribute 'randomized_search_cv'\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\box\\box.py:592\u001b[0m, in \u001b[0;36mbox.box.Box.__getitem__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'randomized_search_cv'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBoxKeyError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\box\\box.py:631\u001b[0m, in \u001b[0;36mbox.box.Box.__getattr__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\box\\box.py:619\u001b[0m, in \u001b[0;36mbox.box.Box.__getitem__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mBoxKeyError\u001b[0m: \"'randomized_search_cv'\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\box\\box.py:633\u001b[0m, in \u001b[0;36mbox.box.Box.__getattr__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ConfigBox' object has no attribute 'randomized_search_cv'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBoxKeyError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\box\\config_box.py:29\u001b[0m, in \u001b[0;36mbox.config_box.ConfigBox.__getattr__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\box\\box.py:647\u001b[0m, in \u001b[0;36mbox.box.Box.__getattr__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mBoxKeyError\u001b[0m: \"'ConfigBox' object has no attribute 'randomized_search_cv'\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\box\\box.py:592\u001b[0m, in \u001b[0;36mbox.box.Box.__getitem__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'randomized_search_cv'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBoxKeyError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\box\\box.py:631\u001b[0m, in \u001b[0;36mbox.box.Box.__getattr__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\box\\box.py:619\u001b[0m, in \u001b[0;36mbox.box.Box.__getitem__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mBoxKeyError\u001b[0m: \"'randomized_search_cv'\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\box\\box.py:633\u001b[0m, in \u001b[0;36mbox.box.Box.__getattr__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ConfigBox' object has no attribute 'randomized_search_cv'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBoxKeyError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mdict\u001b[39m(\u001b[43mparams_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandomized_search_cv\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\box\\config_box.py:31\u001b[0m, in \u001b[0;36mbox.config_box.ConfigBox.__getattr__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\box\\box.py:647\u001b[0m, in \u001b[0;36mbox.box.Box.__getattr__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mBoxKeyError\u001b[0m: \"'ConfigBox' object has no attribute 'randomized_search_cv'\""
     ]
    }
   ],
   "source": [
    "dict(params_config.randomized_search_cv['Random Forest'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('params.yaml', 'r') as yaml_file:\n",
    "    config = yaml.safe_load(yaml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ScannerError",
     "evalue": "while scanning a simple key\n  in \"config/config.yaml\", line 5, column 1\ncould not find expected ':'\n  in \"config/config.yaml\", line 6, column 26",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mScannerError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig/config.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m yaml_file:\n\u001b[1;32m----> 3\u001b[0m     config_ \u001b[38;5;241m=\u001b[39m \u001b[43myaml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43myaml_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m config_\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\yaml\\__init__.py:125\u001b[0m, in \u001b[0;36msafe_load\u001b[1;34m(stream)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msafe_load\u001b[39m(stream):\n\u001b[0;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m    Parse the first YAML document in a stream\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m    and produce the corresponding Python object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m    to be safe for untrusted input.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSafeLoader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\yaml\\__init__.py:81\u001b[0m, in \u001b[0;36mload\u001b[1;34m(stream, Loader)\u001b[0m\n\u001b[0;32m     79\u001b[0m loader \u001b[38;5;241m=\u001b[39m Loader(stream)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_single_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m     loader\u001b[38;5;241m.\u001b[39mdispose()\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\yaml\\constructor.py:49\u001b[0m, in \u001b[0;36mBaseConstructor.get_single_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_single_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Ensure that the stream contains a single document and construct it.\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_single_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstruct_document(node)\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\yaml\\composer.py:36\u001b[0m, in \u001b[0;36mComposer.get_single_node\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     34\u001b[0m document \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_event(StreamEndEvent):\n\u001b[1;32m---> 36\u001b[0m     document \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Ensure that the stream contains no more documents.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_event(StreamEndEvent):\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\yaml\\composer.py:55\u001b[0m, in \u001b[0;36mComposer.compose_document\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_event()\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Compose the root node.\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_node\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Drop the DOCUMENT-END event.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_event()\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\yaml\\composer.py:84\u001b[0m, in \u001b[0;36mComposer.compose_node\u001b[1;34m(self, parent, index)\u001b[0m\n\u001b[0;32m     82\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompose_sequence_node(anchor)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_event(MappingStartEvent):\n\u001b[1;32m---> 84\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_mapping_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mascend_resolver()\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\yaml\\composer.py:133\u001b[0m, in \u001b[0;36mComposer.compose_mapping_node\u001b[1;34m(self, anchor)\u001b[0m\n\u001b[0;32m    129\u001b[0m item_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompose_node(node, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m#if item_key in node.value:\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m#    raise ComposerError(\"while composing a mapping\", start_event.start_mark,\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m#            \"found duplicate key\", key_event.start_mark)\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m item_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m#node.value[item_key] = item_value\u001b[39;00m\n\u001b[0;32m    135\u001b[0m node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mappend((item_key, item_value))\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\yaml\\composer.py:84\u001b[0m, in \u001b[0;36mComposer.compose_node\u001b[1;34m(self, parent, index)\u001b[0m\n\u001b[0;32m     82\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompose_sequence_node(anchor)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_event(MappingStartEvent):\n\u001b[1;32m---> 84\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_mapping_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mascend_resolver()\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\yaml\\composer.py:133\u001b[0m, in \u001b[0;36mComposer.compose_mapping_node\u001b[1;34m(self, anchor)\u001b[0m\n\u001b[0;32m    129\u001b[0m item_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompose_node(node, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m#if item_key in node.value:\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m#    raise ComposerError(\"while composing a mapping\", start_event.start_mark,\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m#            \"found duplicate key\", key_event.start_mark)\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m item_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m#node.value[item_key] = item_value\u001b[39;00m\n\u001b[0;32m    135\u001b[0m node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mappend((item_key, item_value))\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\yaml\\composer.py:64\u001b[0m, in \u001b[0;36mComposer.compose_node\u001b[1;34m(self, parent, index)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompose_node\u001b[39m(\u001b[38;5;28mself\u001b[39m, parent, index):\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAliasEvent\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     65\u001b[0m         event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_event()\n\u001b[0;32m     66\u001b[0m         anchor \u001b[38;5;241m=\u001b[39m event\u001b[38;5;241m.\u001b[39manchor\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\yaml\\parser.py:98\u001b[0m, in \u001b[0;36mParser.check_event\u001b[1;34m(self, *choices)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_event \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate:\n\u001b[1;32m---> 98\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_event \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m choices:\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\yaml\\parser.py:449\u001b[0m, in \u001b[0;36mParser.parse_block_mapping_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_token(ValueToken):\n\u001b[0;32m    448\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_token()\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mKeyToken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mValueToken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBlockEndToken\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    450\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_block_mapping_key)\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_block_node_or_indentless_sequence()\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\yaml\\scanner.py:115\u001b[0m, in \u001b[0;36mScanner.check_token\u001b[1;34m(self, *choices)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_token\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mchoices):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# Check if the next token is one of the given types.\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneed_more_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    116\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetch_more_tokens()\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens:\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\yaml\\scanner.py:152\u001b[0m, in \u001b[0;36mScanner.need_more_tokens\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# The current token may be a potential simple key, so we\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# need to look further.\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstale_possible_simple_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_possible_simple_key() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens_taken:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\yaml\\scanner.py:291\u001b[0m, in \u001b[0;36mScanner.stale_possible_simple_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mline \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline  \\\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m-\u001b[39mkey\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1024\u001b[39m:\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mrequired:\n\u001b[1;32m--> 291\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ScannerError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile scanning a simple key\u001b[39m\u001b[38;5;124m\"\u001b[39m, key\u001b[38;5;241m.\u001b[39mmark,\n\u001b[0;32m    292\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not find expected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_mark())\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpossible_simple_keys[level]\n",
      "\u001b[1;31mScannerError\u001b[0m: while scanning a simple key\n  in \"config/config.yaml\", line 5, column 1\ncould not find expected ':'\n  in \"config/config.yaml\", line 6, column 26"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "with open('config/config.yaml') as yaml_file:\n",
    "    config_ = yaml.safe_load(yaml_file)\n",
    "config_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config['optuna']['Logistic_Regression']\n",
    "# space = {}\n",
    "# for key,value in config['optuna']['Random Forest'].items():\n",
    "#     space[key] = eval(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['optuna']['Logistic_Regression']['penalty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(config['optuna']['Logistic_Regression'].keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['optuna']['Logistic_Regression']['penalty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = Path('F:\\iNeuron\\Projects\\scania_failures_2\\\\artifacts\\data\\processed\\stage_1_processing\\preprocessed_train_data.csv')\n",
    "# test_path = Path('F:\\iNeuron\\Projects\\scania_failures_2\\\\artifacts\\data\\processed\\stage_2_processing\\processed_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    size = input(\"Enter the size\")\n",
    "    if eval(size) == None:\n",
    "        train_df = pd.read_csv(train_path).iloc[:eval(size),:]\n",
    "        print(train_df.shape)\n",
    "    elif eval(size):\n",
    "        train_df = pd.read_csv(train_path).iloc[:eval(size),:]\n",
    "        print(train_df.shape)\n",
    "except:\n",
    "    print(\"Invalid Input!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if size:\n",
    "    print (\"True\")\n",
    "else: \n",
    "    print (\"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa_000</th>\n",
       "      <th>ab_000</th>\n",
       "      <th>ac_000</th>\n",
       "      <th>ad_000</th>\n",
       "      <th>ae_000</th>\n",
       "      <th>af_000</th>\n",
       "      <th>ag_000</th>\n",
       "      <th>ag_001</th>\n",
       "      <th>ag_002</th>\n",
       "      <th>ag_003</th>\n",
       "      <th>...</th>\n",
       "      <th>ee_002</th>\n",
       "      <th>ee_003</th>\n",
       "      <th>ee_004</th>\n",
       "      <th>ee_005</th>\n",
       "      <th>ee_006</th>\n",
       "      <th>ee_007</th>\n",
       "      <th>ee_008</th>\n",
       "      <th>ee_009</th>\n",
       "      <th>ef_000</th>\n",
       "      <th>eg_000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76698.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.130706e+09</td>\n",
       "      <td>280.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1240520.0</td>\n",
       "      <td>493384.0</td>\n",
       "      <td>721044.0</td>\n",
       "      <td>469792.0</td>\n",
       "      <td>339156.0</td>\n",
       "      <td>157956.0</td>\n",
       "      <td>73224.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33058.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>421400.0</td>\n",
       "      <td>178064.0</td>\n",
       "      <td>293306.0</td>\n",
       "      <td>245416.0</td>\n",
       "      <td>133654.0</td>\n",
       "      <td>81140.0</td>\n",
       "      <td>97576.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41040.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.280000e+02</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>277378.0</td>\n",
       "      <td>159812.0</td>\n",
       "      <td>423992.0</td>\n",
       "      <td>409564.0</td>\n",
       "      <td>320746.0</td>\n",
       "      <td>158022.0</td>\n",
       "      <td>95128.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.000000e+01</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>...</td>\n",
       "      <td>240.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60874.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.368000e+03</td>\n",
       "      <td>458.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>622012.0</td>\n",
       "      <td>229790.0</td>\n",
       "      <td>405298.0</td>\n",
       "      <td>347188.0</td>\n",
       "      <td>286954.0</td>\n",
       "      <td>311560.0</td>\n",
       "      <td>433954.0</td>\n",
       "      <td>1218.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>153002.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.640000e+02</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2564.0</td>\n",
       "      <td>...</td>\n",
       "      <td>998500.0</td>\n",
       "      <td>566884.0</td>\n",
       "      <td>1290398.0</td>\n",
       "      <td>1218244.0</td>\n",
       "      <td>1019768.0</td>\n",
       "      <td>717762.0</td>\n",
       "      <td>898642.0</td>\n",
       "      <td>28588.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>2286.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.130707e+09</td>\n",
       "      <td>224.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10578.0</td>\n",
       "      <td>6760.0</td>\n",
       "      <td>21126.0</td>\n",
       "      <td>68424.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>112.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.130706e+09</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>792.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>452.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>2622.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>80292.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.130706e+09</td>\n",
       "      <td>494.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>699352.0</td>\n",
       "      <td>222654.0</td>\n",
       "      <td>347378.0</td>\n",
       "      <td>225724.0</td>\n",
       "      <td>194440.0</td>\n",
       "      <td>165070.0</td>\n",
       "      <td>802280.0</td>\n",
       "      <td>388422.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>40222.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.980000e+02</td>\n",
       "      <td>628.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>440066.0</td>\n",
       "      <td>183200.0</td>\n",
       "      <td>344546.0</td>\n",
       "      <td>254068.0</td>\n",
       "      <td>225148.0</td>\n",
       "      <td>158304.0</td>\n",
       "      <td>170384.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows Ã— 171 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aa_000  ab_000        ac_000  ad_000  ae_000  af_000  ag_000  ag_001  \\\n",
       "0       76698.0     NaN  2.130706e+09   280.0     0.0     0.0     0.0     0.0   \n",
       "1       33058.0     NaN  0.000000e+00     NaN     0.0     0.0     0.0     0.0   \n",
       "2       41040.0     NaN  2.280000e+02   100.0     0.0     0.0     0.0     0.0   \n",
       "3          12.0     0.0  7.000000e+01    66.0     0.0    10.0     0.0     0.0   \n",
       "4       60874.0     NaN  1.368000e+03   458.0     0.0     0.0     0.0     0.0   \n",
       "...         ...     ...           ...     ...     ...     ...     ...     ...   \n",
       "59995  153002.0     NaN  6.640000e+02   186.0     0.0     0.0     0.0     0.0   \n",
       "59996    2286.0     NaN  2.130707e+09   224.0     0.0     0.0     0.0     0.0   \n",
       "59997     112.0     0.0  2.130706e+09    18.0     0.0     0.0     0.0     0.0   \n",
       "59998   80292.0     NaN  2.130706e+09   494.0     0.0     0.0     0.0     0.0   \n",
       "59999   40222.0     NaN  6.980000e+02   628.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "       ag_002  ag_003  ...     ee_002    ee_003     ee_004     ee_005  \\\n",
       "0         0.0     0.0  ...  1240520.0  493384.0   721044.0   469792.0   \n",
       "1         0.0     0.0  ...   421400.0  178064.0   293306.0   245416.0   \n",
       "2         0.0     0.0  ...   277378.0  159812.0   423992.0   409564.0   \n",
       "3         0.0   318.0  ...      240.0      46.0       58.0       44.0   \n",
       "4         0.0     0.0  ...   622012.0  229790.0   405298.0   347188.0   \n",
       "...       ...     ...  ...        ...       ...        ...        ...   \n",
       "59995     0.0  2564.0  ...   998500.0  566884.0  1290398.0  1218244.0   \n",
       "59996     0.0     0.0  ...    10578.0    6760.0    21126.0    68424.0   \n",
       "59997     0.0     0.0  ...      792.0     386.0      452.0      144.0   \n",
       "59998     0.0     0.0  ...   699352.0  222654.0   347378.0   225724.0   \n",
       "59999     0.0     0.0  ...   440066.0  183200.0   344546.0   254068.0   \n",
       "\n",
       "          ee_006    ee_007    ee_008    ee_009  ef_000  eg_000  \n",
       "0       339156.0  157956.0   73224.0       0.0     0.0     0.0  \n",
       "1       133654.0   81140.0   97576.0    1500.0     0.0     0.0  \n",
       "2       320746.0  158022.0   95128.0     514.0     0.0     0.0  \n",
       "3           10.0       0.0       0.0       0.0     4.0    32.0  \n",
       "4       286954.0  311560.0  433954.0    1218.0     0.0     0.0  \n",
       "...          ...       ...       ...       ...     ...     ...  \n",
       "59995  1019768.0  717762.0  898642.0   28588.0     0.0     0.0  \n",
       "59996      136.0       0.0       0.0       0.0     0.0     0.0  \n",
       "59997      146.0    2622.0       0.0       0.0     0.0     0.0  \n",
       "59998   194440.0  165070.0  802280.0  388422.0     0.0     0.0  \n",
       "59999   225148.0  158304.0  170384.0     158.0     0.0     0.0  \n",
       "\n",
       "[60000 rows x 171 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 171)\n",
      "(10000, 170) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "train_df = pd.read_csv(train_path).iloc[:10000,:]\n",
    "# test_df = pd.read_csv(test_path).iloc[:35000,:]\n",
    "\n",
    "x_train = train_df.drop(columns='class')\n",
    "y_train = train_df['class']\n",
    "# x_test = test_df.drop(columns='class')\n",
    "# y_test = test_df['class']\n",
    "print(train_df.shape)\n",
    "print(x_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost in fold5: 72580.0\n",
      "Fold:  6\n",
      "(9000, 170) (9000,) (1000, 170) (1000,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total= 1.9min\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.3s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(17586, 170) (17586,) (1954, 170) (1954,)\n",
      "Fitting model\n",
      "Cost in fold6: 33140.0\n",
      "Fold:  7\n",
      "(9000, 170) (9000,) (1000, 170) (1000,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total= 1.6min\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.1s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(17580, 170) (17580,) (1948, 170) (1948,)\n",
      "Fitting model\n",
      "Cost in fold7: 39060.0\n",
      "Fold:  8\n",
      "(9000, 170) (9000,) (1000, 170) (1000,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=  53.4s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.1s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(17592, 170) (17592,) (1956, 170) (1956,)\n",
      "Fitting model\n",
      "Cost in fold8: 12080.0\n",
      "Fold:  9\n",
      "(9000, 170) (9000,) (1000, 170) (1000,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=  54.1s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.1s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(17582, 170) (17582,) (1936, 170) (1936,)\n",
      "Fitting model\n",
      "Cost in fold9: 55050.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-14 17:58:36,761] Trial 2 finished with value: 44341.0 and parameters: {'n_estimators': 247, 'criterion': 'log_loss', 'max_features': None, 'class_weight': 'balanced_subsample'}. Best is trial 1 with value: 33043.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  0\n",
      "(9000, 170) (9000,) (1000, 170) (1000,)\n",
      "Starting pipeline transformation of Xtrain\n",
      "[Pipeline] ....... (step 1 of 2) Processing Knn_imputer, total=  42.4s\n",
      "[Pipeline] ..... (step 2 of 2) Processing Robust_Scaler, total=   0.2s\n",
      "Starting SMOTE transformation of Xtrain,Ytrain\n",
      "Starting pipeline transformation of Xtest\n",
      "Starting SMOTE transformation of Xtest,Ytest\n",
      "(17570, 170) (17570,) (1956, 170) (1956,)\n",
      "Fitting model\n",
      "Cost in fold0: 52660.0\n",
      "Fold:  1\n",
      "(9000, 170) (9000,) (1000, 170) (1000,)\n",
      "Starting pipeline transformation of Xtrain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-01-14 18:00:12,194] Trial 3 failed with parameters: {'n_estimators': 474, 'criterion': 'entropy', 'max_features': 'log2', 'class_weight': 'balanced'} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_16696\\1320443644.py\", line 37, in objective\n",
      "    X_train_transformed = pipeline.fit_transform(X = x_train_, y = y_train_)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\pipeline.py\", line 471, in fit_transform\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\pipeline.py\", line 377, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\joblib\\memory.py\", line 353, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\pipeline.py\", line 957, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 157, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\base.py\", line 919, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 157, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\impute\\_knn.py\", line 365, in transform\n",
      "    for chunk in gen:\n",
      "  File \"f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py\", line 2027, in pairwise_distances_chunked\n",
      "    D_chunk = reduce_func(D_chunk, sl.start)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\impute\\_knn.py\", line 348, in process_chunk\n",
      "    value = self._calc_impute(\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\impute\\_knn.py\", line 183, in _calc_impute\n",
      "    donors_idx = np.argpartition(dist_pot_donors, n_neighbors - 1, axis=1)[\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 858, in argpartition\n",
      "    return _wrapfunc(a, 'argpartition', kth, axis=axis, kind=kind, order=order)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 59, in _wrapfunc\n",
      "    return bound(*args, **kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-01-14 18:00:12,205] Trial 3 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 63\u001b[0m\n\u001b[0;32m     61\u001b[0m pruner\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39mpruners\u001b[38;5;241m.\u001b[39mMedianPruner()\n\u001b[0;32m     62\u001b[0m find_param\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39mcreate_study(storage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmysql://root:qwerty12345@localhost/example\u001b[39m\u001b[38;5;124m'\u001b[39m,load_if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,direction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m, study_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOptunaSearchCV with Pruning\u001b[39m\u001b[38;5;124m'\u001b[39m, pruner\u001b[38;5;241m=\u001b[39mpruner)\n\u001b[1;32m---> 63\u001b[0m \u001b[43mfind_param\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[24], line 37\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial, data)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_train_\u001b[38;5;241m.\u001b[39mshape, y_train_\u001b[38;5;241m.\u001b[39mshape, x_test_\u001b[38;5;241m.\u001b[39mshape, y_test_\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting pipeline transformation of Xtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m X_train_transformed \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_train_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_train_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting SMOTE transformation of Xtrain,Ytrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m X_train_smote,y_train_smote \u001b[38;5;241m=\u001b[39m smote\u001b[38;5;241m.\u001b[39mfit_resample(X \u001b[38;5;241m=\u001b[39m X_train_transformed,y \u001b[38;5;241m=\u001b[39m y_train_)\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\pipeline.py:471\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model and transform with the final estimator.\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \n\u001b[0;32m    446\u001b[0m \u001b[38;5;124;03mFits all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m    Transformed samples.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    470\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m--> 471\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m last_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\pipeline.py:377\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    375\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\joblib\\memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\pipeline.py:957\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 957\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    959\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\base.py:919\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m--> 919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\impute\\_knn.py:365\u001b[0m, in \u001b[0;36mKNNImputer.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;66;03m# process in fixed-memory chunks\u001b[39;00m\n\u001b[0;32m    357\u001b[0m gen \u001b[38;5;241m=\u001b[39m pairwise_distances_chunked(\n\u001b[0;32m    358\u001b[0m     X[row_missing_idx, :],\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    363\u001b[0m     reduce_func\u001b[38;5;241m=\u001b[39mprocess_chunk,\n\u001b[0;32m    364\u001b[0m )\n\u001b[1;32m--> 365\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# process_chunk modifies X in place. No return value.\u001b[39;49;00m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mpass\u001b[39;49;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_empty_features:\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2027\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[1;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[0;32m   2025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2026\u001b[0m     chunk_size \u001b[38;5;241m=\u001b[39m D_chunk\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 2027\u001b[0m     D_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mreduce_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2028\u001b[0m     _check_chunk_size(D_chunk, chunk_size)\n\u001b[0;32m   2029\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m D_chunk\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\impute\\_knn.py:348\u001b[0m, in \u001b[0;36mKNNImputer.transform.<locals>.process_chunk\u001b[1;34m(dist_chunk, start)\u001b[0m\n\u001b[0;32m    343\u001b[0m     dist_subset \u001b[38;5;241m=\u001b[39m dist_chunk[dist_idx_map[receivers_idx] \u001b[38;5;241m-\u001b[39m start][\n\u001b[0;32m    344\u001b[0m         :, potential_donors_idx\n\u001b[0;32m    345\u001b[0m     ]\n\u001b[0;32m    347\u001b[0m n_neighbors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_neighbors, \u001b[38;5;28mlen\u001b[39m(potential_donors_idx))\n\u001b[1;32m--> 348\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calc_impute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdist_subset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_X\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpotential_donors_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_fit_X\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpotential_donors_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m X[receivers_idx, col] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\sklearn\\impute\\_knn.py:183\u001b[0m, in \u001b[0;36mKNNImputer._calc_impute\u001b[1;34m(self, dist_pot_donors, n_neighbors, fit_X_col, mask_fit_X_col)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper function to impute a single column.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m    Imputed values for receiver.\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# Get donors\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m donors_idx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist_pot_donors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\n\u001b[0;32m    184\u001b[0m     :, :n_neighbors\n\u001b[0;32m    185\u001b[0m ]\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# Get weight matrix from distance matrix\u001b[39;00m\n\u001b[0;32m    188\u001b[0m donors_dist \u001b[38;5;241m=\u001b[39m dist_pot_donors[\n\u001b[0;32m    189\u001b[0m     np\u001b[38;5;241m.\u001b[39marange(donors_idx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])[:, \u001b[38;5;28;01mNone\u001b[39;00m], donors_idx\n\u001b[0;32m    190\u001b[0m ]\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:858\u001b[0m, in \u001b[0;36margpartition\u001b[1;34m(a, kth, axis, kind, order)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_argpartition_dispatcher)\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21margpartition\u001b[39m(a, kth, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintroselect\u001b[39m\u001b[38;5;124m'\u001b[39m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    781\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;124;03m    Perform an indirect partition along the given axis using the\u001b[39;00m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;124;03m    algorithm specified by the `kind` keyword. It returns an array of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    856\u001b[0m \n\u001b[0;32m    857\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 858\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margpartition\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\iNeuron\\Projects\\scania_failures_2\\scania_truck\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from src.utils import eval_metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from imblearn.combine import SMOTETomek\n",
    "# from imblearn.pipeline import Pipeline\n",
    "# from src.config.configuration_manager import ConfigurationManager\n",
    "# obj = ConfigurationManager()\n",
    "skf = StratifiedKFold(n_splits=10,shuffle= False, random_state=None)\n",
    "def objective(trial, data = train_df):\n",
    "  # train_x,test_x,y_train,y_test=train_test_split(data,target,test_size=0.20,random_state=25)\n",
    "  # param= {list(config['optuna']['ExtraTreesClassifier'].keys())[0] : eval(config['optuna']['ExtraTreesClassifier']['penalty'])}\n",
    "\n",
    "  # preprocessor_config = obj.get_preprocessor_config()\n",
    "  # schema = load_yaml(obj.schema)\n",
    "  # target = list(schema.Target.keys())[0]\n",
    "  pipeline = Pipeline(steps=[('Knn_imputer',KNNImputer()),\n",
    "                             ('Robust_Scaler',RobustScaler())],\n",
    "                            verbose=True)\n",
    "  smote = SMOTETomek(n_jobs=-1,sampling_strategy='minority',random_state=8)\n",
    "  X = data.drop(columns='class')\n",
    "  y = data['class']\n",
    "  space = {}\n",
    "  score = []\n",
    "  for key,value in config['optuna']['ExtraTreesClassifier'].items():\n",
    "    space[key] = eval(value)\n",
    "  for fold, (train_indices, test_indices) in enumerate(skf.split(X, y)):\n",
    "    print (\"Fold: \",fold)\n",
    "    x_train_ = data.drop(columns = 'class').iloc[train_indices]\n",
    "    y_train_ = data['class'].iloc[train_indices]\n",
    "    x_test_  = data.drop(columns = 'class').iloc[test_indices]\n",
    "    y_test_  = data['class'].iloc[test_indices]\n",
    "\n",
    "    print(x_train_.shape, y_train_.shape, x_test_.shape, y_test_.shape)\n",
    "    print(\"Starting pipeline transformation of Xtrain\")\n",
    "    X_train_transformed = pipeline.fit_transform(X = x_train_, y = y_train_)\n",
    "    print(\"Starting SMOTE transformation of Xtrain,Ytrain\")\n",
    "    X_train_smote,y_train_smote = smote.fit_resample(X = X_train_transformed,y = y_train_)\n",
    "\n",
    "    print(\"Starting pipeline transformation of Xtest\")\n",
    "    X_test_transformed = pipeline.transform(X = x_test_)\n",
    "    print(\"Starting SMOTE transformation of Xtest,Ytest\")\n",
    "    X_test_smote,y_test_smote = smote.fit_resample(X = X_test_transformed,y = y_test_)\n",
    "\n",
    "\n",
    "    print(X_train_smote.shape, y_train_smote.shape, X_test_smote.shape, y_test_smote.shape)\n",
    "    log_reg=ExtraTreesClassifier(**space)\n",
    "    print(\"Fitting model\")\n",
    "    log_reg.fit(X_train_smote,y_train_smote)\n",
    "    y_predict=log_reg.predict(X_test_smote)\n",
    "    cost = eval_metrics(y_true = y_test_smote , y_pred = y_predict)\n",
    "    print (f\"Cost in fold{fold}: {cost}\")\n",
    "    trial.report(cost, fold)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "    else:\n",
    "       score.append(cost)\n",
    "  return np.mean(score)\n",
    "\n",
    "pruner=optuna.pruners.MedianPruner()\n",
    "find_param=optuna.create_study(storage='mysql://root:qwerty12345@localhost/example',\n",
    "                               load_if_exists=True,direction = \"minimize\",\n",
    "                               study_name = 'OptunaSearchCV with Pruning', \n",
    "                               pruner=pruner)\n",
    "find_param.optimize(objective,n_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.storages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33097.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_param.best_trial.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_class_weight</th>\n",
       "      <th>params_criterion</th>\n",
       "      <th>params_max_features</th>\n",
       "      <th>params_n_estimators</th>\n",
       "      <th>system_attrs_completed_rung_0</th>\n",
       "      <th>system_attrs_completed_rung_1</th>\n",
       "      <th>system_attrs_completed_rung_2</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>33097.0</td>\n",
       "      <td>2024-01-14 15:00:07.599244</td>\n",
       "      <td>2024-01-14 15:07:40.859962</td>\n",
       "      <td>0 days 00:07:33.260718</td>\n",
       "      <td>balanced</td>\n",
       "      <td>entropy</td>\n",
       "      <td>log2</td>\n",
       "      <td>260</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>34479.0</td>\n",
       "      <td>2024-01-14 15:07:40.867896</td>\n",
       "      <td>2024-01-14 15:16:08.830188</td>\n",
       "      <td>0 days 00:08:27.962292</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>entropy</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>343</td>\n",
       "      <td>26550.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>33244.0</td>\n",
       "      <td>2024-01-14 15:16:08.835107</td>\n",
       "      <td>2024-01-14 15:25:31.428111</td>\n",
       "      <td>0 days 00:09:22.593004</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>log2</td>\n",
       "      <td>429</td>\n",
       "      <td>13620.0</td>\n",
       "      <td>54620.0</td>\n",
       "      <td>28570.0</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15600.0</td>\n",
       "      <td>2024-01-14 15:25:31.431102</td>\n",
       "      <td>2024-01-14 15:27:25.583525</td>\n",
       "      <td>0 days 00:01:54.152423</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>375</td>\n",
       "      <td>15600.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>14130.0</td>\n",
       "      <td>2024-01-14 15:27:25.585447</td>\n",
       "      <td>2024-01-14 15:29:52.244854</td>\n",
       "      <td>0 days 00:02:26.659407</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>gini</td>\n",
       "      <td>log2</td>\n",
       "      <td>961</td>\n",
       "      <td>14130.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>17090.0</td>\n",
       "      <td>2024-01-14 15:29:52.247768</td>\n",
       "      <td>2024-01-14 15:34:14.847344</td>\n",
       "      <td>0 days 00:04:22.599576</td>\n",
       "      <td>balanced</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>647</td>\n",
       "      <td>17090.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>17100.0</td>\n",
       "      <td>2024-01-14 15:34:14.848285</td>\n",
       "      <td>2024-01-14 15:38:53.651309</td>\n",
       "      <td>0 days 00:04:38.803024</td>\n",
       "      <td>balanced</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>None</td>\n",
       "      <td>705</td>\n",
       "      <td>17100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>18130.0</td>\n",
       "      <td>2024-01-14 15:38:53.653227</td>\n",
       "      <td>2024-01-14 15:40:40.120447</td>\n",
       "      <td>0 days 00:01:46.467220</td>\n",
       "      <td>balanced</td>\n",
       "      <td>gini</td>\n",
       "      <td>log2</td>\n",
       "      <td>865</td>\n",
       "      <td>18130.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>43095.0</td>\n",
       "      <td>2024-01-14 15:40:40.121450</td>\n",
       "      <td>2024-01-14 16:08:36.728736</td>\n",
       "      <td>0 days 00:27:56.607286</td>\n",
       "      <td>balanced</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>None</td>\n",
       "      <td>907</td>\n",
       "      <td>57080.0</td>\n",
       "      <td>54550.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>34560.0</td>\n",
       "      <td>2024-01-14 16:08:36.731746</td>\n",
       "      <td>2024-01-14 16:18:27.913535</td>\n",
       "      <td>0 days 00:09:51.181789</td>\n",
       "      <td>balanced_subsample</td>\n",
       "      <td>gini</td>\n",
       "      <td>sqrt</td>\n",
       "      <td>889</td>\n",
       "      <td>34560.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PRUNED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number    value             datetime_start          datetime_complete  \\\n",
       "0       0  33097.0 2024-01-14 15:00:07.599244 2024-01-14 15:07:40.859962   \n",
       "1       1  34479.0 2024-01-14 15:07:40.867896 2024-01-14 15:16:08.830188   \n",
       "2       2  33244.0 2024-01-14 15:16:08.835107 2024-01-14 15:25:31.428111   \n",
       "3       3  15600.0 2024-01-14 15:25:31.431102 2024-01-14 15:27:25.583525   \n",
       "4       4  14130.0 2024-01-14 15:27:25.585447 2024-01-14 15:29:52.244854   \n",
       "5       5  17090.0 2024-01-14 15:29:52.247768 2024-01-14 15:34:14.847344   \n",
       "6       6  17100.0 2024-01-14 15:34:14.848285 2024-01-14 15:38:53.651309   \n",
       "7       7  18130.0 2024-01-14 15:38:53.653227 2024-01-14 15:40:40.120447   \n",
       "8       8  43095.0 2024-01-14 15:40:40.121450 2024-01-14 16:08:36.728736   \n",
       "9       9  34560.0 2024-01-14 16:08:36.731746 2024-01-14 16:18:27.913535   \n",
       "\n",
       "                duration params_class_weight params_criterion  \\\n",
       "0 0 days 00:07:33.260718            balanced          entropy   \n",
       "1 0 days 00:08:27.962292  balanced_subsample          entropy   \n",
       "2 0 days 00:09:22.593004  balanced_subsample         log_loss   \n",
       "3 0 days 00:01:54.152423  balanced_subsample         log_loss   \n",
       "4 0 days 00:02:26.659407  balanced_subsample             gini   \n",
       "5 0 days 00:04:22.599576            balanced          entropy   \n",
       "6 0 days 00:04:38.803024            balanced         log_loss   \n",
       "7 0 days 00:01:46.467220            balanced             gini   \n",
       "8 0 days 00:27:56.607286            balanced         log_loss   \n",
       "9 0 days 00:09:51.181789  balanced_subsample             gini   \n",
       "\n",
       "  params_max_features  params_n_estimators  system_attrs_completed_rung_0  \\\n",
       "0                log2                  260                            NaN   \n",
       "1                sqrt                  343                        26550.0   \n",
       "2                log2                  429                        13620.0   \n",
       "3                sqrt                  375                        15600.0   \n",
       "4                log2                  961                        14130.0   \n",
       "5                None                  647                        17090.0   \n",
       "6                None                  705                        17100.0   \n",
       "7                log2                  865                        18130.0   \n",
       "8                None                  907                        57080.0   \n",
       "9                sqrt                  889                        34560.0   \n",
       "\n",
       "   system_attrs_completed_rung_1  system_attrs_completed_rung_2     state  \n",
       "0                            NaN                            NaN  COMPLETE  \n",
       "1                            NaN                            NaN  COMPLETE  \n",
       "2                        54620.0                        28570.0  COMPLETE  \n",
       "3                            NaN                            NaN    PRUNED  \n",
       "4                            NaN                            NaN    PRUNED  \n",
       "5                            NaN                            NaN    PRUNED  \n",
       "6                            NaN                            NaN    PRUNED  \n",
       "7                            NaN                            NaN    PRUNED  \n",
       "8                        54550.0                            NaN  COMPLETE  \n",
       "9                            NaN                            NaN    PRUNED  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_param.trials_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 260,\n",
       " 'criterion': 'entropy',\n",
       " 'max_features': 'log2',\n",
       " 'class_weight': 'balanced'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_param.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_param.trials_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt.pyll.base import scope\n",
    "import hyperopt\n",
    "a = scope.int(hp.quniform('n_estimators', 100, 2000, 1))\n",
    "\n",
    "print(hyperopt.pyll.stochastic.sample(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['hyperopt']['Random Forest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['hyperopt']['Random Forest'].items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, space_eval\n",
    "from hyperopt.pyll.base import scope\n",
    "space = {}\n",
    "for key,value in config['hyperopt']['Random Forest'].items():\n",
    "    space[key] = eval(value)\n",
    "def objective(space):\n",
    "    hp_rand_forest = RandomForestClassifier(**space)\n",
    "    hp_rand_forest.fit(train_x,y_train)\n",
    "    y_pred = hp_rand_forest.predict(test_x)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true=y_test, y_pred=y_pred).ravel()\n",
    "    cost = float((10*fp)+(500*fn))\n",
    "    return cost\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn= objective,\n",
    "            space= space,\n",
    "            algo= tpe.suggest,\n",
    "            max_evals = 10,\n",
    "            trials= trials)\n",
    "best_randf=space_eval(space,best)\n",
    "best_randf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials.average_best_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_randf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = {'n_estimators': 352,\n",
    " 'criterion': 'entropy',\n",
    " 'max_features': 'log2',\n",
    " 'class_weight': 'balanced_subsample'}\n",
    "randf=RandomForestClassifier(**b)\n",
    "randf.fit(train_x,y_train)\n",
    "y_pred=randf.predict(test_x)\n",
    "tn, fp, fn, tp = confusion_matrix(y_true=y_test, y_pred=y_pred).ravel()\n",
    "cost = float((10*fp)+(500*fn))\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "model = XGBClassifier(njobs = -1)\n",
    "model_name = 'XGB_Classifier'\n",
    "params_config = load_yaml(PARAMS_PATH)\n",
    "\n",
    "# def cost_scorer(estimator, X, y):\n",
    "#     x_train_, x_test_, y_train_, y_test_ = train_test_split(X,y, random_state=8, test_size = 0.25)\n",
    "#     estimator.fit(x_train_,y_train_)\n",
    "#     y_pred = estimator.predict(x_test_)\n",
    "#     tn, fp, fn, tp = confusion_matrix(y_true=y_test_, y_pred=y_pred).ravel()\n",
    "#     cost = float((10*fp)+(500*fn))\n",
    "#     return cost\n",
    "\n",
    "params_grid = dict(params_config.randomized_search_cv[model_name])\n",
    "random_search_cv = RandomizedSearchCV(estimator = model,\n",
    "                                        param_distributions = params_grid,\n",
    "                                        n_iter = 5,\n",
    "                                        scoring = 'accuracy',\n",
    "                                        n_jobs = -1,\n",
    "                                        verbose = 3,\n",
    "                                        random_state = 8\n",
    "                                        )\n",
    "random_search_cv.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(**random_search_cv.best_params_)\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_true=y_test, y_pred=y_pred).ravel()\n",
    "cost = float((10*fp)+(500*fn))\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the User Defined Function: hyper_parameter_tuning()\n",
    "This is done to call optuna and hyperopt at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import parameter_tuning\n",
    "report = parameter_tuning(model_class = SGDClassifier,\n",
    "                          model_name = 'SGD_Classifier',\n",
    "                          x_train = x_train,\n",
    "                          x_test = x_test,\n",
    "                          y_train = y_train,\n",
    "                          y_test = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = {'Logistic_Regression': {'Optuna': {'cost': 437200.0, 'params': {'penalty': None}}, 'HyperOpt': {'cost': 437200.0, 'params': {'penalty': None}}, 'Best_Params': {'penalty': None}, 'Cost': 437200.0}, 'SGD_Classifier': {'Optuna': {'cost': 31760.0, 'params': {'loss': 'squared_error'}}, 'HyperOpt': {'cost': 4840.0, 'params': {'loss': 'squared_error'}}, 'Best_Params': {'loss': 'squared_error'}, 'Cost': 4840.0}}\n",
    "costs = [value['Cost'] for value in report.values()]\n",
    "min_cost = min(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value = min(report['Optuna']['cost'],report['HyperOpt']['cost'])\n",
    "if min_value == report['Optuna']['cost']:\n",
    "    params = report['Optuna']['params']\n",
    "else:\n",
    "    params = report['HyperOpt']['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {'Logistic_Regression': {'Optuna': {'cost': 437200.0, 'params': {'penalty': None}}, 'HyperOpt': {'cost': 437200.0, 'params': {'penalty': None}}, 'Best_Params': {'penalty': None}, 'Best_Cost': 437200.0}, 'SGD_Classifier': {'Optuna': {'cost': 125050.0, 'params': {'loss': 'log_loss'}}, 'HyperOpt': \n",
    "{'cost': 15060.0, 'params': {'loss': 'hinge'}}, 'Best_Params': {'loss': 'hinge'}, 'Best_Cost': 15060.0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary['SGD_Classifier'].keys()\n",
    "costs = [value['Best_Cost'] for value in dictionary.values()]\n",
    "min_cost = min(costs)\n",
    "min_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'Logistic_Regression': LogisticRegression(), \n",
    "            'SGD_Classifier': SGDClassifier(),\n",
    "            'Random Forest': RandomForestClassifier, \n",
    "            'Ada_Boost': AdaBoostClassifier, \n",
    "            'Grad_Boost': GradientBoostingClassifier, \n",
    "            'Bagging_Classifier': BaggingClassifier, \n",
    "            'ExtraTreesClassifier': ExtraTreesClassifier, \n",
    "            'Hist_Grad_Boost_Classifier': HistGradientBoostingClassifier, \n",
    "            'Decision_Tree_Classifier': DecisionTreeClassifier,\n",
    "            'XGB_Classifier': XGBClassifier,\n",
    "            'KNN_Classifier': KNeighborsClassifier,\n",
    "            'MLP_Classifier': MLPClassifier\n",
    "            }\n",
    "for i in dictionary.keys():\n",
    "    if min_cost == dictionary[i]['Best_Cost']:\n",
    "        best_params_so_far = dictionary[i]['Best_Params']\n",
    "print(best_params_so_far)\n",
    "\n",
    "\n",
    "##Same code as a list comprehension\n",
    "best_params_so_far_ = [(i, min_cost, dictionary[i]['Best_Params']) for i in dictionary.keys() if min_cost == dictionary[i]['Best_Cost']]\n",
    "print(best_params_so_far_[0])\n",
    "\n",
    "model = models[best_params_so_far_[0][0]]\n",
    "model(**best_params_so_far_[0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fetching the dataframe of 1st five best models for Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_ = sorted(dictionary.items(), key = lambda x: x[1]['Best_Cost'])[:5]\n",
    "best_models = [best_models_[i][0] for i in range(len(best_models_))]\n",
    "best_models_with_params = []\n",
    "for i in best_models:\n",
    "    best_models_with_params.append((i,dictionary[i]['Best_Params']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_with_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_with_params[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_with_params[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimators = {}\n",
    "for i in range(len(best_models_with_params)):\n",
    "    best_estimators[best_models_with_params[i][0]] = models[best_models_with_params[i][0]](**best_models_with_params[i][1])\n",
    "best_estimators = list(zip(best_estimators.keys(),best_estimators.values()))\n",
    "best_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': None,\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'auto',\n",
       " 'n_jobs': None,\n",
       " 'penalty': None,\n",
       " 'random_state': None,\n",
       " 'solver': 'lbfgs',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression(penalty=None).get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimators[1][1].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_with_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {}\n",
    "len(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {'Logistic_Regression': {'Optuna': {'cost': 437200.0, 'params': {'penalty': None}}, 'HyperOpt': {'cost': 437200.0, 'params': {'penalty': None}}, 'Best_Params': {'penalty': None}, 'Best_Cost': 437200.0}}\n",
    "my_dict.values()\n",
    "# [value['Best_Cost'] for value in my_dict.values()]\n",
    "for i in my_dict.values():\n",
    "    value = i['Best_Cost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i['Best_Cost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[('KNN_Classifier', 90240.0, {'n_neighbors': 11, 'weights': 'uniform', 'algorithm': 'brute'})][0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config_Manager - Sorting Report Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config.configuration_manager import ConfigurationManager\n",
    "obj = ConfigurationManager()\n",
    "preprocessor_config = obj.get_preprocessor_config()\n",
    "os.listdir(preprocessor_config.root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.exists(preprocessor_config.preprocessor_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "metrics['name'] = 'LogisticRegression()'\n",
    "metrics['b_score'] = 9999999999.32\n",
    "metrics['a_score'] = 3.23\n",
    "\n",
    "report = {}\n",
    "report['log_reg'] = metrics\n",
    "\n",
    "metrics_a={}\n",
    "metrics_a['name'] = 'SVC()'\n",
    "metrics_a['b_score'] = 9934\n",
    "metrics_a['a_score'] = 23534.23\n",
    "\n",
    "report['svm'] = metrics_a\n",
    "\n",
    "name = max(report.keys(), key = lambda k: report[k].get('a_score', 0))\n",
    "report[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_yaml,load_binary\n",
    "\n",
    "model_dict = load_yaml(filepath = Path('artifacts\\metrics\\metrics.yaml'))\n",
    "\n",
    "model = load_binary(filepath = Path('artifacts\\model\\model.joblib'))\n",
    "\n",
    "preprocessor = load_binary(filepath = Path('artifacts\\preprocessor\\preprocessor.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('artifacts\\data\\processed\\stage_1_processing\\preprocessed_test_data.csv')\n",
    "X = df.drop(columns = 'class').iloc[:1000,:]\n",
    "y = df['class'].iloc[:1000]\n",
    "X_transformed = preprocessor.transform(X = X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_transformed).isna().sum().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "\n",
    "smote = SMOTETomek(sampling_strategy = 'minority',\n",
    "                   random_state = 8,\n",
    "                   n_jobs = -1)\n",
    "\n",
    "x_smote, y_smote = smote.fit_resample(X = X_transformed,\n",
    "                                      y = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (balanced_accuracy_score, f1_score,\n",
    "                             accuracy_score, confusion_matrix)\n",
    "\n",
    "y_pred = model.predict(x_smote)\n",
    "balanced_accuracy_score_ = float(balanced_accuracy_score(y_true=y_smote, y_pred=y_pred))\n",
    "f1_score_ = float(f1_score(y_true=y_smote, y_pred=y_pred))\n",
    "accuracy_score_ = float(accuracy_score(y_true=y_smote, y_pred=y_pred))\n",
    "tn, fp, fn, tp = confusion_matrix(y_true=y_smote, y_pred=y_pred).ravel()\n",
    "cost_ = float((10*fp)+(500*fn))\n",
    "\n",
    "print(f'balanced_accuracy_score_: {balanced_accuracy_score_}')\n",
    "print(f'accuracy_score_: {accuracy_score_}')\n",
    "print(f'f1_score_: {f1_score_}')\n",
    "print(f'cost_: {cost_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DT = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT.predict(x_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import save_yaml\n",
    "save_yaml(file=report,filepath='f:\\iNeuron\\Projects\\Scania_Truck_Failures\\sample.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('f:\\\\iNeuron\\\\Projects\\\\Scania_Truck_Failures')\n",
    "with open (Path('.\\\\config\\\\config.yaml')) as file:\n",
    "    config = yaml.safe_load(file)\n",
    "print({list(config['data_ingestion_config']['test_data3'].keys())[0]:list(config['data_ingestion_config']['test_data3'].values())[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['data_ingestion_config']['test_data3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any(os.listdir('../notebooks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../artifacts\\data\\processed\\stage_1_processing\\preprocessed_train_data.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='class').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('F:\\iNeuron\\Projects\\Data\\Scania Truck Failures\\\\aps_failure_training_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['class'] = df1['class'].map({'neg':0,'pos':1})\n",
    "df1.replace('na',np.nan,inplace=True)\n",
    "col_list = [i for i in df1.columns if i != 'class']\n",
    "for i in col_list:\n",
    "    df1[i]=df1[i].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df = df1.drop(columns = 'class').describe()==df.drop(columns='class').describe()\n",
    "for i in check_df.columns:\n",
    "    print(i,check_df[i].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../artifacts\\data\\processed\\stage_1_processing\\preprocessed_test_data.csv')\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df1 = pd.read_csv('F:\\iNeuron\\Projects\\Data\\Scania Truck Failures\\\\aps_failure_test_set.csv')\n",
    "test_df1['class'] = test_df1['class'].map({'neg':0,'pos':1})\n",
    "test_df1.replace('na',np.nan,inplace=True)\n",
    "col_list = [i for i in test_df1.columns if i != 'class']\n",
    "for i in col_list:\n",
    "    test_df1[i]=test_df1[i].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_check_df = test_df.drop(columns = 'class').describe()==test_df1.drop(columns='class').describe()\n",
    "for i in test_check_df.columns:\n",
    "    print(i,test_check_df[i].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df1['ci_000'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['ci_000'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['ci_000'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ci_000'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"artifacts\\data\\processed\\stage_2_processing\\processed_train_data.csv\").iloc[:1000,:]\n",
    "test_df = pd.read_csv(\"artifacts\\data\\processed\\stage_2_processing\\processed_test_data.csv\").iloc[:1000,:]\n",
    "\n",
    "x_train = train_df.drop(columns = 'class')\n",
    "y_train = train_df['class']\n",
    "x_test = test_df.drop(columns = 'class')\n",
    "y_test = test_df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape,y_train.shape,x_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'Logistic_Regression': LogisticRegression(), \n",
    "        'SGD_Classifier': SGDClassifier(),\n",
    "        'Random Forest': RandomForestClassifier(), \n",
    "        'Ada_Boost': AdaBoostClassifier(), \n",
    "        'Grad_Boost': GradientBoostingClassifier(), \n",
    "        'Bagging_Classifier': BaggingClassifier(), \n",
    "        'ExtraTreesClassifier': ExtraTreesClassifier(), \n",
    "        'Hist_Grad_Boost_Classifier': HistGradientBoostingClassifier(), \n",
    "        'Decision_Tree_Classifier': DecisionTreeClassifier(),\n",
    "        'XGB_Classifier': XGBClassifier(),\n",
    "        'KNN_Classifier': KNeighborsClassifier(),\n",
    "        'MLP_Classifier': MLPClassifier()\n",
    "        }\n",
    "report = {}\n",
    "# metrics_list = ['balanced_accuracy_score','f1_score','accuracy_score']\n",
    "for model_name,model in models.items():\n",
    "        print(model_name)\n",
    "        model.fit(x_train,y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "        \n",
    "        metrics = {}\n",
    "        metrics['balanced_accuracy_score'] = float(balanced_accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "        metrics['f1_score'] = float(f1_score(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "        metrics['accuracy_score'] = float(accuracy_score(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true=y_test, y_pred=y_pred).ravel()\n",
    "        metrics['cost'] = float((10*fp)+(500*fn))\n",
    "\n",
    "        print(metrics,'\\n')\n",
    "\n",
    "        report[model_name] = metrics\n",
    "\n",
    "best_model_name_by_cost = min(report.keys(), key = lambda k: report[k].get('cost', 0))\n",
    "print(\"\\nBest Model:\")\n",
    "print(models[best_model_name_by_cost])\n",
    "print(report[best_model_name_by_cost],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df = pd.DataFrame(report).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df.sort_values(by ='cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = sorted(report.items(), key=lambda x: x[1]['cost'])[:5]\n",
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_names_ = [best_models[i][0] for i in range(len(best_models))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimators = {}\n",
    "for i in best_models_names_:\n",
    "    best_estimators[i] = models[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimators_copy = best_estimators.copy()\n",
    "best_estimators_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimators_copy = list(zip(best_estimators_copy.keys(),best_estimators_copy.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_classifier = StackingClassifier(estimators = best_estimators_copy,\n",
    "                                        final_estimator =  AdaBoostClassifier(),\n",
    "                                        cv = 5,\n",
    "                                        n_jobs = -1,\n",
    "                                        passthrough = False,\n",
    "                                        verbose = 3)\n",
    "\n",
    "stacked_classifier.fit(x_train,y_train)\n",
    "y_pred = stacked_classifier.predict(x_test)\n",
    "\n",
    "balanced_accuracy_score_ = balanced_accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "f1_score_= f1_score(y_true=y_test, y_pred=y_pred)\n",
    "accuracy_score_ = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true=y_test, y_pred=y_pred).ravel()\n",
    "cost_ = (10*fp)+(500*fn)\n",
    "\n",
    "print(f'balanced_accuracy_score_: {balanced_accuracy_score_}')\n",
    "print(f'f1_score_: {f1_score_}')\n",
    "print(f'accuracy_score_: {accuracy_score_}')\n",
    "print(f'cost_: {cost_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_classifier_ = VotingClassifier(estimators = best_estimators_copy,\n",
    "                                      voting = \"soft\",\n",
    "                                    weights = None,\n",
    "                                    n_jobs = -1,\n",
    "                                    verbose = True)\n",
    "voting_classifier_.fit(x_train,y_train)\n",
    "y_pred = voting_classifier_.predict(x_test)\n",
    "\n",
    "balanced_accuracy_score_voting = balanced_accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "f1_score_voting= f1_score(y_true=y_test, y_pred=y_pred)\n",
    "accuracy_score_voting = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true=y_test, y_pred=y_pred).ravel()\n",
    "cost_voting = (10*fp)+(500*fn)\n",
    "\n",
    "print(f'balanced_accuracy_score_: {balanced_accuracy_score_voting}')\n",
    "print(f'f1_score_: {f1_score_voting}')\n",
    "print(f'accuracy_score_: {accuracy_score_voting}')\n",
    "print(f'cost_: {cost_voting}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('.\\\\artifacts\\data\\processed\\stage_1_processing\\preprocessed_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('F:\\\\iNeuron\\Projects\\Data\\Scania Truck Failures\\\\aps_failure_test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.replace('na',np.nan,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop(columns = 'class').isna().sum() == df.drop(columns = 'class').isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_list = df1.drop(columns = 'class').columns.tolist()\n",
    "for i in cols_list:\n",
    "    df1[i] = df1[i].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df1 = df1.drop(columns = 'class').describe() == df.drop(columns = 'class').describe()\n",
    "for i in check_df1.columns:\n",
    "    print(i, check_df1[i].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('.\\\\artifacts\\data\\processed\\stage_1_processing\\preprocessed_train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('F:\\\\iNeuron\\Projects\\Data\\Scania Truck Failures\\\\aps_failure_training_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.replace('na',np.nan,inplace = True)\n",
    "test_col_list = [i for i in df3.columns if i != 'class']\n",
    "for i in test_col_list:\n",
    "    df3[i]=df3[i].astype('float')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(df3.drop(columns = 'class').isna().sum() == df2.drop(columns = 'class').isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_df2 = df3.drop(columns = 'class').describe() == df2.drop(columns = 'class').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in check_df2.columns:\n",
    "    print(i, check_df2[i].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
